{
    "contents" : "#loading libs\nlibrary(\"dplyr\", lib.loc=\"D:/Program Files/R/R-devel/library\")\nlibrary(\"readr\", lib.loc=\"D:/Program Files/R/R-devel/library\")\nlibrary(\"utils\", lib.loc=\"D:/Program Files/R/R-devel/library\")\nlibrary(\"data.table\", lib.loc=\"D:/Program Files/R/R-devel/library\")\n\n\n#swapping dir to get some data\nsetwd(\"../crawler\")\n\n#importing csv (to be ignored)\n\n      # building up the filename\n    \n        #scanning ../crawler for names\n        #f_name <- list.files(\"../crawler\", pattern=\"*.csv\", full.names=FALSE) %>%\n    \n        #extracting date from f_names in numeric format -> YYMMDD\n        #gsub(pattern = \"jobs\\\\.*\\\\.csv$\",replacement = \"\", f_names) %>%\n    \n        #selecting latest dataset\n        #max(na.rm = TRUE)\n  \n      #exeptions to be considered:\n      # - YY, MM and DD shouldn't exceed current date\n      # - MM shouldn't exceed 12\n      # - DD shouldn't exceed number of days specific for a given MM\n      # solution - change string to date?\n\n\n    \n  #actual imported data without \"NA\" elements\n  pracuj_data <- read_csv(\"pracuj.csv\" , col_names = TRUE) %>%\n  filter(position != \"NA\")\n  \n  \n  # switching back to filtering directory\n  setwd(\"../Filtr\")\n  \n  # creating dictionary forjob offers selection\n  \n  #created dictionaries:\n  # - phrase_dic_eng.csv contains summary of phrases in english which determine if\n  #   the job offer belongs to data.science category\n  # - phrase_dic_pl.csv contains summary of phrases in polish which determine\n  #   if the job offer belongs to data.science category\n  # - exeptions_phrase_eng.csv, exeptions_phrase_pl.csv contain expressions which indicate job offer outside data.science industry\n  #\n  # - exeptions_words_eng.csv, exeptions_words_pl.csv -- || -- (words instead of full expressions)  \n  \n  \n  \n  # changing dir to get dicts\n  setwd(\"dict\")\n  \n  # listing all avaliable dictionaries\n  f_dic_names <- list.files(\"../dict\", pattern=\"*.csv\", full.names=FALSE)\n\n  \n  \n  \n  dic_list <- list()\n  \n  \n  #reading all avaliable dictionaries\n  for (dic in f_dic_names) {\n   \n   \n   dic_list_i <- read_csv(paste0(dic) , col_names = TRUE)\n   dic_list <- append(dic_list, dic_list_i)  \n  }\n  \n  # going back to ../\n  setwd(\"../\")\n  \n  #creating propper vector names for phrases extraction from dictionaries to vectors\n  f_dic_names <- lapply(f_dic_names, function (x) {gsub(pattern = \"\\\\.*\\\\.csv$\",replacement = \"\", x)})\n  \n  \n  # naming vector of dictionaries names\n  names(dic_list) <- f_dic_names\n\n\n# Propper filtering\n  \n# 'href' selected for filtering due to universal structure .../position-name-city\n\n  \n  # creating vector used to filter interesting offers\n  needed_complete_phrases <- unlist(dic_list[grep(pattern = \".*\\\\phrase_dic\\\\.*\", names(dic_list))], use.names = FALSE)\n  \n  # creating vector used to filter out exeptions (offers containing \"data-analyst\" etc, but not in data.science industry)\n  exeptions_phrases <- unlist(dic_list[grep(pattern = \".*exeptions_phrase\\\\.*\", names(dic_list))], use.names = FALSE)\n  \n\n  # filtering according to phrases normally indicating data.science industry job   \n  filtered_data <- data.frame()\n  filtered_data_w_dupli <- data.frame()\n  omited_data <- data.frame()\n  for (NCP in needed_complete_phrases)  {\n    filtered_data1 <- mutate(pracuj_data, DSIndicator = grepl(paste0(\".*\",NCP,\".*\"), href) )%>% filter(DSIndicator == TRUE)#%>%mutate(JobName = paste0(NCP)) \n    filtered_data <- rbind(filtered_data, filtered_data1)\n  }\n  \n   \n    nonDS_primarily_omited_data <- mutate(pracuj_data, DSIndicator = grepl(paste0(\".*\",NCP,\".*\"), href) )%>%filter(DSIndicator == FALSE)\n    filtered_data_w_dupli <- filtered_data\n    \n\n  \n  \n  # excluding offers containing phrases which indicate non-data.science affiliation \n    nonDS_exeptions_omited_data <- data.frame()\n  for (EP in exeptions_phrases)\n  {\n    filtered_data <- mutate(filtered_data, ExeptionIndicator = grepl(paste0(\".*\",EP,\".*\"), href) )\n    nonDS_exeptions_omited_data1 <- filter(filtered_data, ExeptionIndicator == TRUE)\n    filtered_data <-filter(filtered_data, ExeptionIndicator == FALSE)\n    nonDS_exeptions_omited_data <- rbind(nonDS_exeptions_omited_data, nonDS_exeptions_omited_data1)\n    }\n\n  \n  \n  \n  # removing \"Indicators\" from dataset\n  filtered_data <- select(filtered_data, -contains(\"Indicator\"))%>%arrange(desc(date))\n  nonDS_primarily_omited_data <- select(nonDS_primarily_omited_data, -contains(\"Indicator\"))%>%arrange(desc(date))\n  nonDS_exeptions_omited_data <- select(nonDS_exeptions_omited_data, -contains(\"Indicator\"))%>%arrange(desc(date))\n  \n  # getting dataset for number of phrases analysis ready\n  filtered_data_w_dupli <- filtered_data\n  \n  # same ID killer\n  filtered_data <- as.data.table(filtered_data)\n  setkey(filtered_data, id)\n  filtered_data <- filtered_data[!duplicated(filtered_data),]\n  filtered_data <- as.data.frame(filtered_data)\n  \n  \n  \n  # writing solution to files\n  write_csv(filtered_data, \"pracuj_filtered.csv\")\n  write_csv( nonDS_exeptions_omited_data, \"nonDS_exeptions_omited_data.csv\")\n  write_csv(nonDS_primarily_omited_data,  \"nonDS_primarily_omited_data.csv\")\n  write_csv(filtered_data_w_dupli, \"filtered_data_w_dupli.csv\")\n  \n  needed_complete_phrases <- as.data.frame(needed_complete_phrases)\n  write_csv(needed_complete_phrases, \"needed_complete_phrases.csv\")\n  \n  exeptions_phrases <- as.data.frame(exeptions_phrases)\n  write_csv(exeptions_phrases, \"exeptions_phrases.csv\")\n  ",
    "created" : 1456088036816.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "263617013",
    "id" : "6477E672",
    "lastKnownWriteTime" : 1456093659,
    "path" : "D:/eR/MI2/pracuj/pracuj/Filtr/FiltrR.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_source"
}